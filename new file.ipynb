import json
import csv

def extract_swagger_info(swagger_file_path):
    with open(swagger_file_path, 'r') as file:
        swagger_data = json.load(file)
        
    paths_info = []
    
    # Extract information for each path
    for path, path_data in swagger_data['paths'].items():
        for method, method_data in path_data.items():
            path_info = {
                'path': path,
                'method': method.upper(),
                'description': method_data.get('description', ''),
                'parameters': '',
                'responses': ''
            }
            
            # Extract parameters
            parameters = method_data.get('parameters', [])
            param_descriptions = []
            for parameter in parameters:
                param_description = f"{parameter['name']} ({parameter.get('in', '')}): {parameter.get('description', '')}"
                param_descriptions.append(param_description)
            path_info['parameters'] = ', '.join(param_descriptions)
            
            # Extract responses
            responses = method_data.get('responses', {})
            response_descriptions = []
            for response_code, response_data in responses.items():
                response_description = f"{response_code}: {response_data.get('description', '')}"
                response_descriptions.append(response_description)
            path_info['responses'] = ', '.join(response_descriptions)
            
            paths_info.append(path_info)
    
    return paths_info

def write_swagger_info_to_csv(swagger_info, csv_file_path):
    fieldnames = ['path', 'method', 'description', 'parameters', 'responses']
    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(swagger_info)

# Example usage:
swagger_file_path = '/path/to/your/swagger_file.json'  # Replace with the actual file path
csv_file_path = '/path/to/your/output_file.csv'  # Replace with the desired output CSV file path

swagger_info = extract_swagger_info(swagger_file_path)
write_swagger_info_to_csv(swagger_info, csv_file_path)



import csv
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2-medium"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def generate_documentation(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')

    # Generate documentation using the model
    output_ids = model.generate(input_ids, max_length=500, num_return_sequences=1, temperature=0.7)
    generated_doc = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return generated_doc

def generate_documentation_from_csv(csv_file_path):
    with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        for row in reader:
            input_text = f"Path: {row['path']}\nMethod: {row['method']}\nDescription: {row['description']}\nParameters: {row['parameters']}\nResponses: {row['responses']}"
            generated_documentation = generate_documentation(input_text)
            # Print or save the generated documentation as needed
            print("Generated Documentation:")
            print(generated_documentation)

# Example usage:
csv_file_path = '/path/to/your/csv_file.csv'  # Replace with the actual CSV file path
generate_documentation_from_csv(csv_file_path)







from here :- 

import os

# Mount Google Drive if necessary
# Only required if your data is stored in Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the path to the folder containing your training data
folder_path = '/content/drive/MyDrive/training_data_folder'

# Function to prepare training data
def prepare_training_data(folder_path):
    training_data = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if os.path.isfile(file_path):
            # Read the file and extract necessary information
            with open(file_path, 'r') as file:
                # Example: Read contents from the file and add to training data
                swagger_data = file.read()
                documentation = generate_documentation(swagger_data)  # Implement your own function
                training_data.append((swagger_data, documentation))
    return training_data

# Prepare training data
training_data = prepare_training_data(folder_path)

# Example usage
for swagger_data, documentation in training_data:
    print("Swagger Data:", swagger_data)
    print("Documentation:", documentation)




from transformers import GPT2Tokenizer, GPT2LMHeadModel
from torch.utils.data import Dataset, DataLoader
import torch
import os

# Define your dataset class
class SwaggerDocumentationDataset(Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # Implement your data processing logic here
        # Each item should contain a pair of (swagger_file, documentation)
        swagger_file, documentation = self.data[idx]
        return {"swagger_file": swagger_file, "documentation": documentation}

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2-medium"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Fine-tuning setup
# Define your fine-tuning parameters, optimizer, learning rate scheduler, etc.

# Prepare your dataset
# Replace this with your actual dataset
training_data = [
    ("example_swagger1.json", "Documentation for example_swagger1"),
    ("example_swagger2.json", "Documentation for example_swagger2"),
    # Add more data samples as needed
]

# Tokenize the dataset
tokenized_dataset = [tokenizer(pair[0], text_pair=pair[1], return_tensors='pt') for pair in training_data]

# Define your dataloader
train_dataset = SwaggerDocumentationDataset(tokenized_dataset)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Fine-tuning loop
for epoch in range(num_epochs):
    for batch in train_dataloader:
        # Implement your fine-tuning logic here
        # Forward pass, compute loss, backward pass, update model parameters

# Save the fine-tuned model
output_dir = '/path/to/save/fine_tuned_model'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)







from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the fine-tuned model and tokenizer
output_dir = '/path/to/save/fine_tuned_model'
tokenizer = GPT2Tokenizer.from_pretrained(output_dir)
model = GPT2LMHeadModel.from_pretrained(output_dir)

# Define a function to generate documentation from Swagger file
def generate_documentation(swagger_file):
    # Tokenize the input Swagger file
    input_ids = tokenizer.encode(swagger_file, return_tensors='pt')
    
    # Generate documentation using the fine-tuned model
    output_ids = model.generate(input_ids, max_length=500, num_return_sequences=1)
    
    # Decode the generated output
    generated_doc = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    return generated_doc

# Example usage:
swagger_file = "example_swagger.json"  # Replace with the path to your Swagger file
generated_documentation = generate_documentation(swagger_file)
print("Generated Documentation:")
print(generated_documentation)



# Fine-tuning loop
num_epochs = 3
for epoch in range(num_epochs):
    model.train()  # Set the model to train mode
    
    # Initialize total loss for this epoch
    total_loss = 0.0
    
    # Iterate over the training dataloader
    for batch in train_dataloader:
        # Move batch to device (GPU if available)
        batch = {k: v.to(device) for k, v in batch.items()}
        
        # Forward pass
        outputs = model(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"], labels=batch["labels"])
        loss = outputs.loss  # Get the loss from the model's output
        
        # Compute gradients
        loss.backward()
        
        # Accumulate the total loss for this epoch
        total_loss += loss.item()
        
        # Update model parameters
        optimizer.step()
        
        # Clear gradients
        optimizer.zero_grad()
        
    # Calculate the average loss for this epoch
    avg_loss = total_loss / len(train_dataloader)
    
    # Print the average loss for monitoring purposes
    print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")
